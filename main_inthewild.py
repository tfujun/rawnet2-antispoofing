import argparse
import sys
import os
import data_utils_LA
import numpy as np
from torch import Tensor
from torch.utils.data import DataLoader
from torchvision import transforms
import yaml
import torch
from torch import nn
from model import RawNet
from tensorboardX import SummaryWriter
import data_utils_InTheWild




def keras_lr_decay(step, decay = 0.0001):
	return 1./(1.+decay*step)

def pad(x, max_len=8075):
    
    x_len = x.shape[0]
    if x_len >= max_len:
        return x[:max_len]
    # need to pad
    num_repeats = int(max_len / x_len)+1
    padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]
    
    return padded_x 

def init_weights(m):
    #print(m)
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform(m.weight)
        m.bias.data.fill_(0.0001)
    elif isinstance(m, nn.BatchNorm1d):
        pass
    else:
        if hasattr(m, 'weight'):
            torch.nn.init.kaiming_normal_(m.weight, a=0.01)
        else:		
            print('no weight',m)


def evaluate_accuracy(data_loader, model, device):
    num_correct = 0.0
    num_total = 0.0
    model.eval()
    for batch_x, batch_y, batch_meta in data_loader:
        
        batch_size = batch_x.size(0)
        num_total += batch_size
        batch_x = batch_x.to(device)
        batch_y = batch_y.view(-1).type(torch.int64).to(device)
        batch_out = model(batch_x,batch_y)
        _, batch_pred = batch_out.max(dim=1)
        num_correct += (batch_pred == batch_y).sum(dim=0).item()
    return 100 * (num_correct / num_total)


def produce_evaluation_file(dataset, model, device, save_path):
    data_loader = DataLoader(dataset, batch_size=8, shuffle=False)
    num_total = 0.0
    model.eval()

    # Initialize lists to store results
    fname_list = []
    key_list = []
    sys_id_list = []
    score_list = []

    # Iterate through the data loader and handle batch contents
    for batch in data_loader:
        # Handle case where batch contains only two elements (no metadata)
        if len(batch) == 2:
            batch_x, batch_y = batch
            batch_meta = None  # No metadata provided
        else:
            batch_x, batch_y, batch_meta = batch

        # Process the batch
        batch_size = batch_x.size(0)
        num_total += batch_size

        # Move data to the correct device
        batch_x = batch_x.to(device)
        batch_y = batch_y.view(-1).type(torch.int64).to(device)

        # Get model output
        batch_out = model(batch_x, batch_y, is_test=True)
        batch_score = batch_out[:, 1].data.cpu().numpy().ravel()

        # Collect results (Handle case where batch_meta might be None)
        if batch_meta:
            fname_list.extend(list(batch_meta[1]))
            key_list.extend(['bonafide' if key == 1 else 'spoof' for key in list(batch_meta[4])])
            sys_id_list.extend([dataset.sysid_dict_inv[s.item()] for s in list(batch_meta[3])])

        score_list.extend(batch_score.tolist())

    # Save results to a file
    with open(save_path, 'w') as fh:
        for f, s, k, cm in zip(fname_list, sys_id_list, key_list, score_list):
            if dataset.is_eval:
                fh.write(f'{f} {s} {k} {cm}\n')
            else:
                fh.write(f'{f} {cm}\n')

    print(f'Result saved to {save_path}')



def train_epoch(data_loader, model, lr,optim, device):
    running_loss = 0
    num_correct = 0.0
    num_total = 0.0
    ii = 0
    model.train()
    weight = torch.FloatTensor([1.0, 9.0]).to(device)
    criterion = nn.CrossEntropyLoss(weight=weight)
    
    for batch_x, batch_y, batch_meta in train_loader:
       
        batch_size = batch_x.size(0)
        num_total += batch_size
        ii += 1
        batch_x = batch_x.to(device)
        batch_y = batch_y.view(-1).type(torch.int64).to(device)
        batch_out = model(batch_x,batch_y)
        batch_loss = criterion(batch_out, batch_y)
        _, batch_pred = batch_out.max(dim=1)
        num_correct += (batch_pred == batch_y).sum(dim=0).item()
        running_loss += (batch_loss.item() * batch_size)
        if ii % 10 == 0:
            sys.stdout.write('\r \t {:.2f}'.format(
                (num_correct/num_total)*100))
        optim.zero_grad()
        batch_loss.backward()
        optim.step()
       
    running_loss /= num_total
    train_accuracy = (num_correct/num_total)*100
    return running_loss, train_accuracy




if __name__ == '__main__':
    parser = argparse.ArgumentParser('ASVSpoof2019  model')
    parser.add_argument('--eval', action='store_true', default=False,
                        help='eval mode')
    parser.add_argument('--model_path', type=str,
                        default=None, help='Model checkpoint')
   
    parser.add_argument('--database_path', type=str, default='../release_in_the_wild/', help='change this to user\'s full directory address of LA database')
    parser.add_argument('--protocols_path', type=str, default='../release_in_the_wild/meta.csv', help='Change with path to user\'s LA database protocols directory address')
    parser.add_argument('--eval_output', type=str, default=None,
                        help='Path to save the evaluation result')
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--num_epochs', type=int, default=100)
    parser.add_argument('--lr', type=float, default=0.0001)
    parser.add_argument('--weight_decay', type=float, default=0.0001)
    parser.add_argument('--comment', type=str, default=None,
                        help='Comment to describe the saved mdoel')
    parser.add_argument('--track', type=str, default='logical')
    parser.add_argument('--features', type=str, default='Raw_audio')
    parser.add_argument('--is_eval', action='store_true', default=False)
    parser.add_argument('--eval_part', type=int, default=0)
    parser.add_argument('--loss', type=str, default='weighted_CCE')
    

    dir_yaml = os.path.splitext('model_config_RawNet2')[0] + '.yaml'

    with open(dir_yaml, 'r') as f_yaml:
            parser1 = yaml.safe_load(f_yaml)

    
    np.random.seed(parser1['seed'])
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


    if not os.path.exists('models'):
        os.mkdir('models')
    args = parser.parse_args()

    # LA and PA
    track = args.track
    
    #Creat Model
    model_tag = 'model_{}_{}_{}_{}_{}'.format(
        track, args.loss, args.num_epochs, args.batch_size, args.lr)
    if args.comment:
        model_tag = model_tag + '_{}'.format(args.comment)
    model_save_path = os.path.join('models', model_tag)
    
    is_logical = (track == 'logical')
    if not os.path.exists(model_save_path):
        os.mkdir(model_save_path)
    
    
    transforms = transforms.Compose([
        
        lambda x: pad(x),
        lambda x: Tensor(x)
        
    ])

    # GPU device
    device = 'cuda' if torch.cuda.is_available() else 'cpu'                  # cuda-0
    
    

    # Dataloader
    dev_set = data_utils_InTheWild.InTheWildDataset(csv_path=args.protocols_path, data_path=args.database_path, transform=transforms)
    
    wild_loader = DataLoader(dev_set, batch_size=args.batch_size, shuffle=True)
    #torch.backends.cudnn.enabled = False
    
    # Model Initialization
    if bool(parser1['mg']):
            model_1gpu = RawNet(parser1['model'], device)
            nb_params = sum([param.view(-1).size()[0] for param in model_1gpu.parameters()])
            model =(model_1gpu).to(device)
    else:
        model = RawNet(parser1['model'], device).to(device)
        nb_params = sum([param.view(-1).size()[0] for param in model.parameters()])
    

    # Adam optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr,weight_decay=args.weight_decay)
    
    
    if args.model_path:
        model.load_state_dict(torch.load(args.model_path,map_location=device))
        print('Model loaded : {}'.format(args.model_path))

    

    if args.eval:
        
        produce_evaluation_file(dev_set, model, device, args.eval_output)
        sys.exit(0)


    # Dataloader
    train_set = data_utils_LA.ASVDataset(database_path=args.database_path,protocols_path=args.protocols_path,is_train=True, is_logical=is_logical, transform=transforms,
                                      feature_name=args.features)
    train_loader = DataLoader(
        train_set, batch_size=args.batch_size, shuffle=True)

    

    # Training and validation 
    num_epochs = args.num_epochs
    writer = SummaryWriter('logs/{}'.format(model_tag))
    best_acc = 99
    for epoch in range(num_epochs):
        running_loss, train_accuracy = train_epoch(train_loader,model, args.lr,optimizer, device)
        valid_accuracy = evaluate_accuracy(dev_loader, model, device)
        writer.add_scalar('train_accuracy', train_accuracy, epoch)
        writer.add_scalar('valid_accuracy', valid_accuracy, epoch)
        writer.add_scalar('loss', running_loss, epoch)
        print('\n{} - {} - {:.2f} - {:.2f}'.format(epoch,
                                                   running_loss, train_accuracy, valid_accuracy))
        print('*'*50)
        print('dev_acc %f', valid_accuracy)
        if valid_accuracy > best_acc:
            print('best model found at epoch', epoch)
        print('*'*50)
        best_acc = max(valid_accuracy, best_acc)
        torch.save(model.state_dict(), os.path.join(model_save_path, 'epoch_{}.pth'.format(epoch)))
